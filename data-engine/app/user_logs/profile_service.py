"""
ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„± ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
GPT_recommend.mdì˜ ì¦ë¶„ ê°±ì‹  ê³µì‹ êµ¬í˜„
"""

import asyncio
import logging
import numpy as np
import uuid
from typing import Dict, List
from ..core.database import get_database, get_collection_name, get_url_hash
from ..vectorization.embeddings import embedding_service
from ..vectorization.qdrant_client import QdrantService

logger = logging.getLogger(__name__)

# 17ê°œ ì¹´í…Œê³ ë¦¬ ì •ì˜ (setup_categories.pyì™€ ë™ì¼)
CATEGORIES = [
    "ì •ì¹˜", "ì‚¬íšŒ", "ê²½ì œ", "ê¸°ìˆ ", "ê³¼í•™", "ê±´ê°•",
    "êµìœ¡", "ë¬¸í™”", "ì—”í„°í…Œì¸ë¨¼íŠ¸", "ìŠ¤í¬ì¸ ", "ì—­ì‚¬",
    "í™˜ê²½", "ì—¬í–‰", "ìƒí™œ", "ê°€ì •", "ì¢…êµ", "ì² í•™"
]


class UserProfileService:
    """ì‚¬ìš©ì í”„ë¡œí•„ ë²¡í„° ìƒì„± ë° ê´€ë¦¬"""

    def __init__(self):
        self.qdrant_service = QdrantService()
        self.user_locks = {}  # ì‚¬ìš©ìë³„ asyncio.Lock

    def _calculate_weight(self, data: dict) -> float:
        """ì‚¬ìš©ì í–‰ë™ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚° (visitCount ê°€ì¤‘ê°€ì¤‘ì¹˜ ë¬¸ì œ í•´ê²°)"""
        base_weight = 1.0

        # ì²´ë¥˜ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (30ì´ˆ ì´ìƒì´ë©´ ê°€ì¤‘ì¹˜ ì¦ê°€)
        time_spent = data.get('timeSpent', 0)
        if time_spent > 30:
            base_weight += min((time_spent - 30) / 60, 2.0)  # ìµœëŒ€ 3.0

        # ìŠ¤í¬ë¡¤ ê¹Šì´ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (50% ì´ìƒì´ë©´ ê°€ì¤‘ì¹˜ ì¦ê°€)
        scroll_depth = data.get('maxScrollDepth', 0)
        if scroll_depth > 50:
            base_weight += (scroll_depth - 50) / 100  # ìµœëŒ€ 0.5 ì¶”ê°€

        # visitCountëŠ” ë©”íƒ€ë°ì´í„°ë¡œë§Œ ì €ì¥, ê°€ì¤‘ì¹˜ ê³„ì‚°ì—ì„œ ì œì™¸ (ê°€ì¤‘ê°€ì¤‘ì¹˜ ë¬¸ì œ ë°©ì§€)
        # ëŒ€ì‹  íˆìŠ¤í† ë¦¬ ë°ì´í„°ì˜ ê²½ìš° typedCount ë¹„ìœ¨ì„ í™œìš©
        if data.get('dataType') == 'history':
            typed_count = data.get('typedCount', 0)
            visit_count = data.get('visitCount', 1)
            typed_ratio = typed_count / max(visit_count, 1)

            if typed_ratio > 0.3:  # 30% ì´ìƒ ì§ì ‘ íƒ€ì´í•‘
                base_weight += 0.5  # ì˜ë„ì„± ë³´ë„ˆìŠ¤

        return round(base_weight, 3)

    def _prepare_text_for_embedding(self, data: dict) -> str:
        """ë¸Œë¼ìš°ì§•/íˆìŠ¤í† ë¦¬ ë°ì´í„°ì—ì„œ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        combined_text = ""

        if 'content' in data:
            content = data['content']
            clean_title = content.get('cleanTitle', '').strip()
            clean_content = content.get('cleanContent', '')[:1500].strip()

            if clean_title and clean_content:
                combined_text = f"{clean_title} {clean_content}"
            elif clean_title:
                combined_text = clean_title
            elif clean_content:
                combined_text = clean_content

        return combined_text.strip()

    async def _classify_and_update_categories(self, valid_data: List[Dict], vectors: List[List[float]], collection):
        """ë²¡í„° ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë° MongoDB ì—…ë°ì´íŠ¸"""
        logger.info(f"[ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜] {len(vectors)}ê°œ URL ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹œì‘")

        try:
            # 17ê°œ ì¹´í…Œê³ ë¦¬ ë²¡í„° ê°€ì ¸ì˜¤ê¸° (setup_categories.pyë¡œ ë¯¸ë¦¬ ì €ì¥ëœ ê²ƒ)
            category_vectors = await self._get_category_vectors()

            if not category_vectors:
                logger.warning("[ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜] ì¹´í…Œê³ ë¦¬ ë²¡í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. setup_categories.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return

            # ê° URL ë²¡í„°ì™€ ì¹´í…Œê³ ë¦¬ ë²¡í„° ë¹„êµ
            categories_assigned = []
            bulk_updates = []

            for i, (data, vector) in enumerate(zip(valid_data, vectors)):
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = []
                for category, cat_vector in category_vectors.items():
                    similarity = self._cosine_similarity(vector, cat_vector)
                    similarities.append((category, similarity))

                # ê°€ì¥ ìœ ì‚¬í•œ ì¹´í…Œê³ ë¦¬ ì„ íƒ
                best_category, best_score = max(similarities, key=lambda x: x[1])
                categories_assigned.append(best_category)

                # MongoDB ì—…ë°ì´íŠ¸ ì¿¼ë¦¬ ì¤€ë¹„
                bulk_updates.append({
                    "_id": data["_id"],
                    "category": best_category,
                    "category_score": round(best_score, 3)
                })

                if i < 5:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê¹…
                    logger.info(f"  URL: {data.get('url', '')[:50]}... â†’ {best_category} (ìœ ì‚¬ë„: {best_score:.3f})")

            # MongoDB ë²Œí¬ ì—…ë°ì´íŠ¸
            if bulk_updates:
                await self._bulk_update_categories(collection, bulk_updates)

            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            for cat in categories_assigned:
                category_stats[cat] = category_stats.get(cat, 0) + 1

            logger.info(f"[ì¹´í…Œê³ ë¦¬ í†µê³„] {dict(sorted(category_stats.items(), key=lambda x: x[1], reverse=True))}")

        except Exception as e:
            logger.error(f"[ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì—ëŸ¬] {e}")

    async def _get_category_vectors(self) -> Dict[str, List[float]]:
        """Qdrantì—ì„œ 17ê°œ ì¹´í…Œê³ ë¦¬ ë²¡í„° ê°€ì ¸ì˜¤ê¸°"""
        try:
            category_vectors = {}
            for i, category in enumerate(CATEGORIES):
                point_id = i  # ì •ìˆ˜ ID ì‚¬ìš© (0, 1, 2, ...)
                point = await self.qdrant_service.get_point("user_logs", point_id)
                if point and point.get("vector"):
                    category_vectors[category] = point["vector"]

            logger.info(f"[ì¹´í…Œê³ ë¦¬ ë²¡í„°] {len(category_vectors)}ê°œ ì¹´í…Œê³ ë¦¬ ë²¡í„° ë¡œë“œ ì™„ë£Œ")
            return category_vectors

        except Exception as e:
            logger.error(f"[ì¹´í…Œê³ ë¦¬ ë²¡í„° ë¡œë“œ ì—ëŸ¬] {e}")
            return {}

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        vec1_np = np.array(vec1)
        vec2_np = np.array(vec2)

        dot_product = np.dot(vec1_np, vec2_np)
        norm1 = np.linalg.norm(vec1_np)
        norm2 = np.linalg.norm(vec2_np)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    async def _bulk_update_categories(self, collection, bulk_updates: List[Dict]):
        """MongoDB ë²Œí¬ ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸"""
        try:
            from pymongo import UpdateOne

            operations = []
            for update in bulk_updates:
                operations.append(UpdateOne(
                    {"_id": update["_id"]},
                    {"$set": {
                        "category": update["category"],
                        "categoryScore": update["category_score"]
                    }}
                ))

            if operations:
                result = await collection.bulk_write(operations)
                logger.info(f"[MongoDB ì—…ë°ì´íŠ¸] {result.modified_count}ê°œ ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"[MongoDB ì—…ë°ì´íŠ¸ ì—ëŸ¬] {e}")

    async def create_initial_profile_from_history(self, user_id: str, limit: int = 500) -> Dict:
        """íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¡œë¶€í„° ì´ˆê¸° user_profile ìƒì„±"""
        logger.info(f"[í”„ë¡œí•„ ìƒì„±] ì‚¬ìš©ì {user_id}ì˜ ì´ˆê¸° í”„ë¡œí•„ ìƒì„± ì‹œì‘")

        try:
            # MongoDBì—ì„œ íˆìŠ¤í† ë¦¬ ë°ì´í„° ì¡°íšŒ
            database = get_database()
            collection_name = get_collection_name(user_id, 'history')
            collection = database[collection_name]

            logger.info(f"[ë°ì´í„° ì¡°íšŒ] ìƒ¤ë“œ {collection_name}ì—ì„œ ìµœëŒ€ {limit}ê°œ ì¡°íšŒ")
            cursor = collection.find({"userId": user_id}).sort("savedAt", -1).limit(limit)
            history_data = await cursor.to_list(length=limit)

            if not history_data:
                return {"success": False, "message": "íˆìŠ¤í† ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

            logger.info(f"[ë°ì´í„° ìˆ˜ì§‘] {len(history_data)}ê°œ íˆìŠ¤í† ë¦¬ ë°ì´í„° ìˆ˜ì§‘")

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„ë² ë”© ìƒì„±
            texts_for_embedding = []
            weights = []
            valid_data = []

            for data in history_data:
                combined_text = self._prepare_text_for_embedding(data)

                if combined_text.strip():  # ë¹ˆ ì½˜í…ì¸ ë§Œ ì œì™¸
                    texts_for_embedding.append(combined_text)
                    weights.append(self._calculate_weight(data))
                    valid_data.append(data)

            if not texts_for_embedding:
                return {"success": False, "message": "ì„ë² ë”© ê°€ëŠ¥í•œ íˆìŠ¤í† ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

            logger.info(f"[ì„ë² ë”© ìƒì„±] {len(texts_for_embedding)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹œì‘")

            # ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ GMS API í¬ê¸° ì œí•œ í•´ê²°)
            chunk_size = 20
            vectors = []
            for i in range(0, len(texts_for_embedding), chunk_size):
                chunk = texts_for_embedding[i:i + chunk_size]
                logger.info(f"  â†’ {i+1}~{min(i+chunk_size, len(texts_for_embedding))}ë²ˆì§¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...")
                chunk_vectors = await embedding_service.encode_batch(chunk)
                vectors.extend(chunk_vectors)

            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë° MongoDB ì—…ë°ì´íŠ¸
            await self._classify_and_update_categories(valid_data, vectors, collection)

            # GPT_recommend.md ì¦ë¶„ ê³µì‹: ê°€ì¤‘í‰ê· ìœ¼ë¡œ í”„ë¡œí•„ ë²¡í„° ê³„ì‚°
            profile_vector, total_weight = self._calculate_weighted_average(vectors, weights)

            # user_logs ì»¬ë ‰ì…˜ì— ê°œë³„ ë¡œê·¸ ì €ì¥ (URL í•´ì‹œ ê¸°ë°˜)
            await self._save_user_logs_to_qdrant(user_id, valid_data, vectors, weights)

            # user_profiles ì»¬ë ‰ì…˜ì— í”„ë¡œí•„ ë²¡í„° ì €ì¥
            await self._save_user_profile_to_qdrant(user_id, profile_vector, total_weight, len(vectors), weights)

            logger.info(f"[ì™„ë£Œ] ì‚¬ìš©ì {user_id} í”„ë¡œí•„ ìƒì„± ì™„ë£Œ - ì´ ê°€ì¤‘ì¹˜: {total_weight}")

            # íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œ í›„ ë¸Œë¼ìš°ì§• ë°ì´í„° ì¼ê´„ ì²˜ë¦¬
            await self._process_all_browsing_data(user_id)

            return {
                "success": True,
                "message": "ì´ˆê¸° ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ",
                "user_id": user_id,
                "processed_count": len(vectors),
                "total_weight": total_weight,
                "profile_vector_dim": len(profile_vector)
            }

        except Exception as e:
            logger.error(f"[ì—ëŸ¬] í”„ë¡œí•„ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"success": False, "message": f"í”„ë¡œí•„ ìƒì„± ì‹¤íŒ¨: {str(e)}"}

    def _calculate_weighted_average(self, vectors: List[List[float]], weights: List[float]) -> tuple:
        """ê°€ì¤‘í‰ê· ìœ¼ë¡œ í”„ë¡œí•„ ë²¡í„° ê³„ì‚°"""
        vectors_array = np.array(vectors)
        weights_array = np.array(weights)

        # ê°€ì¤‘í‰ê· : (v1*w1 + v2*w2 + ...) / (w1 + w2 + ...)
        weighted_sum = np.sum(vectors_array * weights_array.reshape(-1, 1), axis=0)
        total_weight = np.sum(weights_array)

        profile_vector = weighted_sum / total_weight

        # L2 ì •ê·œí™”
        profile_vector = profile_vector / np.linalg.norm(profile_vector)

        return profile_vector.tolist(), float(total_weight)

    async def _save_user_logs_to_qdrant(self, user_id: str, data_list: List[Dict], vectors: List[List[float]], weights: List[float]):
        """user_logs ì»¬ë ‰ì…˜ì— ê°œë³„ ë¡œê·¸ ë²¡í„° ì €ì¥ (URL í•´ì‹œ ê¸°ë°˜)"""
        try:
            collection_name = "user_logs"

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadatas = []
            point_ids = []

            for i, (data, weight) in enumerate(zip(data_list, weights)):
                url = data.get('url', '')
                url_hash = get_url_hash(url)

                metadata = {
                    "user_id": user_id,
                    "url": url,
                    "url_hash": url_hash,
                    "weight": weight,  # ê°€ì¤‘ì¹˜ëŠ” ë©”íƒ€ë°ì´í„°ì— ì €ì¥
                    "domain": data.get('domain', ''),
                    "title": data.get('content', {}).get('cleanTitle', ''),
                    "data_source": data.get('dataType', 'history'),
                    "visit_count": data.get('visitCount', 1),
                    "saved_at": data.get('savedAt')
                }

                # íˆìŠ¤í† ë¦¬ íŠ¹í™” í•„ë“œ ì¶”ê°€
                if data.get('dataType') == 'history':
                    metadata.update({
                        "typed_count": data.get('typedCount', 0),
                        "total_visits": data.get('totalVisits', 0),
                        "direct_visits": data.get('directVisits', 0),
                        "last_visit_time": data.get('lastVisitTime')
                    })

                metadatas.append(metadata)
                point_ids.append(str(uuid.uuid4()))  # UUIDë¡œ ê³ ìœ  ID ìƒì„±

            # Qdrantì— ì €ì¥ (URL í•´ì‹œ ê¸°ë°˜ Upsert)
            await self.qdrant_service.save_vectors_with_metadata_and_ids(
                collection_name, vectors, metadatas, point_ids
            )

            logger.info(f"[ì €ì¥] user_logs ì»¬ë ‰ì…˜ì— {len(vectors)}ê°œ ë²¡í„° ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"[ì—ëŸ¬] user_logs ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    async def _save_user_profile_to_qdrant(self, user_id: str, profile_vector: List[float], total_weight: float, log_count: int, weights: List[float]):
        """user_profiles ì»¬ë ‰ì…˜ì— í”„ë¡œí•„ ë²¡í„° ì €ì¥"""
        try:
            collection_name = "user_profiles"

            # í†µê³„ ê³„ì‚°
            avg_weight = total_weight / log_count if log_count > 0 else 0.0
            max_weight = max(weights) if weights else 0.0
            min_weight = min(weights) if weights else 0.0

            metadata = {
                "user_id": user_id,
                "weight_sum": total_weight,  # GPT_recommend.mdì˜ W_old
                "log_count": log_count,              # ë°˜ì˜ëœ ë¡œê·¸ ê°œìˆ˜
                "avg_weight": round(avg_weight, 3),   # í‰ê·  ê°€ì¤‘ì¹˜
                "max_weight": max_weight,            # ìµœëŒ€ ê°€ì¤‘ì¹˜
                "min_weight": min_weight,            # ìµœì†Œ ê°€ì¤‘ì¹˜
                "created_from": "history_data"
            }

            # UUIDë¡œ ê³ ìœ  Point ID ìƒì„±
            profile_point_id = str(uuid.uuid4())

            # ë””ë²„ê¹…: ì €ì¥í•  ë°ì´í„° í™•ì¸
            logger.info(f"[ë””ë²„ê·¸] í”„ë¡œí•„ ì €ì¥ ì¤‘: user_id={user_id}")
            logger.info(f"[ë””ë²„ê·¸] profile_vector íƒ€ì…: {type(profile_vector)}, None ì—¬ë¶€: {profile_vector is None}")
            if profile_vector is not None:
                logger.info(f"[ë””ë²„ê·¸] profile_vector ê¸¸ì´: {len(profile_vector)}, ì²˜ìŒ 3ê°œ ê°’: {profile_vector[:3]}")
            logger.info(f"[ë””ë²„ê·¸] total_weight: {total_weight} (íƒ€ì…: {type(total_weight)})")

            await self.qdrant_service.save_vectors_with_metadata_and_ids(
                collection_name, [profile_vector], [metadata], [profile_point_id]
            )

            logger.info(f"[ì €ì¥] user_profiles ì»¬ë ‰ì…˜ì— {user_id} í”„ë¡œí•„ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"[ì—ëŸ¬] user_profiles ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    async def update_profile_with_new_log(self, user_id: str, new_data: Dict) -> Dict:
        """ìƒˆë¡œìš´ ë¸Œë¼ìš°ì§• ë¡œê·¸ë¡œ í”„ë¡œí•„ ì¦ë¶„ ì—…ë°ì´íŠ¸ (GPT_recommend.md ê³µì‹)"""
        # ì‚¬ìš©ìë³„ Lock ìƒì„± ë° ì ìš©
        if user_id not in self.user_locks:
            self.user_locks[user_id] = asyncio.Lock()

        async with self.user_locks[user_id]:
            logger.info(f"[í”„ë¡œí•„ ì—…ë°ì´íŠ¸] ì‚¬ìš©ì {user_id} ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹œì‘")

            try:
                # ìƒˆ ë¡œê·¸ ë²¡í„°í™”
                combined_text = self._prepare_text_for_embedding(new_data)
                if not combined_text.strip():
                    return {"success": False, "message": "ì„ë² ë”© ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}

                v_new = await embedding_service.encode(combined_text)
                w_new = self._calculate_weight(new_data)

                # ê¸°ì¡´ í”„ë¡œí•„ ë²¡í„° ê°€ì ¸ì˜¤ê¸° (ë©”íƒ€ë°ì´í„° í•„í„° ì‚¬ìš©)
                old_profile = await self.qdrant_service.get_user_profile("user_profiles", user_id)

                if not old_profile:
                    # í”„ë¡œí•„ì´ ì—†ìœ¼ë©´ íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¼ (íˆìŠ¤í† ë¦¬ ì¬ì²˜ë¦¬ ë°©ì§€)
                    logger.warning(f"[ê±´ë„ˆë›°ê¸°] ì‚¬ìš©ì {user_id} í”„ë¡œí•„ì´ ì—†ìŠµë‹ˆë‹¤. íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì„¸ìš”.")
                    return {
                        "success": False,
                        "message": "í”„ë¡œí•„ì´ ì—†ìŠµë‹ˆë‹¤. íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì„¸ìš”.",
                        "skipped": True
                    }

                V_old = old_profile["vector"]
                W_old = old_profile["payload"]["weight_sum"]

                # ë””ë²„ê¹…: None ê°’ ì›ì¸ ë¶„ì„
                logger.info(f"[ë””ë²„ê·¸] old_profile êµ¬ì¡°: id={old_profile.get('id')}, vectorëŠ” None: {V_old is None}, payload keys: {list(old_profile.get('payload', {}).keys())}")
                logger.info(f"[ë””ë²„ê·¸] weight_sum ê°’: {W_old} (íƒ€ì…: {type(W_old)})")

                if V_old is None:
                    logger.error(f"[ì—ëŸ¬] V_oldê°€ Noneì…ë‹ˆë‹¤. í”„ë¡œí•„ ì €ì¥ ì‹œ ë²¡í„°ê°€ ì œëŒ€ë¡œ ì €ì¥ë˜ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±")
                if W_old is None:
                    logger.error(f"[ì—ëŸ¬] W_oldê°€ Noneì…ë‹ˆë‹¤. í”„ë¡œí•„ ì €ì¥ ì‹œ weight_sumì´ ì œëŒ€ë¡œ ì €ì¥ë˜ì§€ ì•Šì•˜ì„ ê°€ëŠ¥ì„±")

                # GPT_recommend.md ì¦ë¶„ ê³µì‹: V_new = (V_old * W_old + v_new * w_new) / (W_old + w_new)
                V_old_array = np.array(V_old)
                v_new_array = np.array(v_new)

                V_new = (V_old_array * W_old + v_new_array * w_new) / (W_old + w_new)
                V_new = V_new / np.linalg.norm(V_new)  # L2 ì •ê·œí™”

                # user_logsì— ìƒˆ ë¡œê·¸ ì €ì¥ (UUID ê¸°ë°˜)
                url = new_data.get('url', '')
                url_hash = get_url_hash(url)
                point_id = str(uuid.uuid4())  # UUIDë¡œ ê³ ìœ  ID ìƒì„±

                metadata = {
                    "user_id": user_id,
                    "url": url,
                    "url_hash": url_hash,
                    "weight": w_new,
                    "domain": new_data.get('domain', ''),
                    "title": new_data.get('content', {}).get('cleanTitle', ''),
                    "data_source": new_data.get('dataType', 'browsing'),
                    "visit_count": new_data.get('visitCount', 1),
                    "saved_at": new_data.get('savedAt')
                }

                await self.qdrant_service.save_vectors_with_metadata_and_ids(
                    "user_logs", [v_new], [metadata], [point_id]
                )

                # ê¸°ì¡´ í†µê³„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                old_log_count = old_profile["payload"].get("log_count", 0)
                old_avg_weight = old_profile["payload"].get("avg_weight", 0.0)
                old_max_weight = old_profile["payload"].get("max_weight", 0.0)
                old_min_weight = old_profile["payload"].get("min_weight", 0.0)

                # ìƒˆë¡œìš´ í†µê³„ ê³„ì‚°
                new_log_count = old_log_count + 1
                new_weight_sum = W_old + w_new
                new_avg_weight = new_weight_sum / new_log_count
                new_max_weight = max(old_max_weight, w_new)
                new_min_weight = min(old_min_weight, w_new) if old_min_weight > 0 else w_new

                # user_profiles ì—…ë°ì´íŠ¸
                updated_metadata = {
                    "user_id": user_id,
                    "weight_sum": new_weight_sum,  # GPT_recommend.mdì˜ W_old + w_new
                    "log_count": new_log_count,
                    "avg_weight": round(new_avg_weight, 3),
                    "max_weight": new_max_weight,
                    "min_weight": new_min_weight,
                    "created_from": old_profile["payload"].get("created_from", "history_data"),
                    "last_update": new_data.get('savedAt')
                }

                # ê¸°ì¡´ í”„ë¡œí•„ì˜ Point IDë¥¼ ì‚¬ìš©í•´ì„œ ì—…ë°ì´íŠ¸
                existing_point_id = old_profile["id"]
                await self.qdrant_service.save_vectors_with_metadata_and_ids(
                    "user_profiles", [V_new.tolist()], [updated_metadata], [existing_point_id]
                )

                # Qdrant ì €ì¥ ì™„ë£Œ í›„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ìˆ˜í–‰
                try:
                    from ..core.database import get_database, get_collection_name
                    collection_name = get_collection_name(user_id, "browsing")
                    collection = get_database()[collection_name]
                    await self._classify_and_update_categories([new_data], [v_new], collection)
                    logger.info("[ì™„ë£Œ] ì‹¤ì‹œê°„ ë¸Œë¼ìš°ì§• ë°ì´í„° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì™„ë£Œ")
                except Exception as e:
                    logger.warning(f"[ê²½ê³ ] ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")

                logger.info(f"[ì™„ë£Œ] í”„ë¡œí•„ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì™„ë£Œ - ìƒˆ ê°€ì¤‘ì¹˜: {W_old + w_new}")

                return {
                    "success": True,
                    "message": "í”„ë¡œí•„ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì™„ë£Œ",
                    "user_id": user_id,
                    "old_weight": W_old,
                    "new_weight": w_new,
                    "total_weight": W_old + w_new
                }

            except Exception as e:
                logger.error(f"[ì—ëŸ¬] í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                return {"success": False, "message": f"í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"}


    async def _process_all_browsing_data(self, user_id: str):
        """íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œ í›„ ëª¨ë“  ë¸Œë¼ìš°ì§• ë°ì´í„° ì¼ê´„ ì²˜ë¦¬"""
        try:
            logger.info(f"ğŸ” [í›„ì²˜ë¦¬] ì‚¬ìš©ì {user_id} íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œ í›„ ë¸Œë¼ìš°ì§• ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ì‹œì‘")
            print(f"ğŸ” [í›„ì²˜ë¦¬] ì‚¬ìš©ì {user_id} íˆìŠ¤í† ë¦¬ ì²˜ë¦¬ ì™„ë£Œ í›„ ë¸Œë¼ìš°ì§• ë°ì´í„° ì¼ê´„ ì²˜ë¦¬ ì‹œì‘")

            # í•´ë‹¹ ì‚¬ìš©ìì˜ ëª¨ë“  ë¸Œë¼ìš°ì§• ë°ì´í„° ì¡°íšŒ
            database = get_database()
            collection_name = get_collection_name(user_id, 'browsing')
            collection = database[collection_name]

            # ëª¨ë“  ë¸Œë¼ìš°ì§• ë°ì´í„° ì¡°íšŒ (íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì¤‘ ìŒ“ì¸ ë°ì´í„°)
            browsing_data_cursor = collection.find({
                "userId": user_id
            }).sort("savedAt", 1)  # ì‹œê°„ìˆœ ì •ë ¬

            browsing_data = await browsing_data_cursor.to_list(length=None)

            if not browsing_data:
                logger.info(f"âœ… [ì™„ë£Œ] ì²˜ë¦¬í•  ë¸Œë¼ìš°ì§• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print(f"âœ… [ì™„ë£œ] ì²˜ë¦¬í•  ë¸Œë¼ìš°ì§• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            logger.info(f"ğŸ“¦ [ë°œê²¬] ì²˜ë¦¬í•  ë¸Œë¼ìš°ì§• ë°ì´í„° {len(browsing_data)}ê°œ ë°œê²¬, ì¼ê´„ ì¦ë¶„ ì²˜ë¦¬ ì‹œì‘")
            print(f"ğŸ“¦ [ë°œê²¬] ì²˜ë¦¬í•  ë¸Œë¼ìš°ì§• ë°ì´í„° {len(browsing_data)}ê°œ ë°œê²¬, ì¼ê´„ ì¦ë¶„ ì²˜ë¦¬ ì‹œì‘")

            # ëª¨ë“  ë¸Œë¼ìš°ì§• ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© ì¦ë¶„ ì—…ë°ì´íŠ¸
            success_count = 0
            for data in browsing_data:
                try:
                    result = await self.update_profile_with_new_log(user_id, data)
                    if result.get("success"):
                        success_count += 1
                except Exception as e:
                    logger.warning(f"[ê²½ê³ ] ë¸Œë¼ìš°ì§• ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            logger.info(f"ğŸ¯ [ì™„ë£Œ] ë¸Œë¼ìš°ì§• ë°ì´í„° {success_count}/{len(browsing_data)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
            print(f"ğŸ¯ [ì™„ë£Œ] ë¸Œë¼ìš°ì§• ë°ì´í„° {success_count}/{len(browsing_data)}ê°œ ì²˜ë¦¬ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ [ì—ëŸ¬] ë¸Œë¼ìš°ì§• ë°ì´í„° í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            print(f"âŒ [ì—ëŸ¬] ë¸Œë¼ìš°ì§• ë°ì´í„° í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")


# Dependency Injectionìœ¼ë¡œ ê´€ë¦¬ë˜ë¯€ë¡œ ì „ì—­ ì‹±ê¸€í†¤ ì œê±°
import os
import sys
import urllib.request
import urllib.parse
import json
import datetime
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import re
from email.utils import parsedate_to_datetime
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import sessionmaker
from datetime import datetime
from .models import News

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from news.summarization import get_summarization_service
# from news.vectorizer import NewsVectorizer  # TODO: Qdrant ì €ì¥ ì¬í™œì„±í™” ì‹œ ì£¼ì„ í•´ì œ

# ====== í™˜ê²½ ì„¤ì • ======
load_dotenv()
client_id = os.environ.get('CLIENT_ID')
client_secret = os.environ.get('CLIENT_SECRET')

CATEGORY_MAP = {
    "ì •ì¹˜": 1,
    "ì‚¬íšŒ": 2,
    "ê²½ì œ": 3,
    "ê¸°ìˆ ": 4,
    "ê³¼í•™": 5,
    "ê±´ê°•": 6,
    "êµìœ¡": 7,
    "ë¬¸í™”": 8,
    "ì—°ì˜ˆ": 9, 
    "ìŠ¤í¬ì¸ ": 10,
    "ì—­ì‚¬": 11,
    "í™˜ê²½": 12,
    "ì—¬í–‰": 13,
    "ìƒí™œ": 14,
    "ê°€ì •": 15,
    "ì¢…êµ": 16,
    "ì² í•™": 17,
}

CATEGORIES = {
    "ì •ì¹˜": ["ì •ë¶€", "êµ­íšŒ", "ëŒ€í†µë ¹", "ì´ë¦¬", "ì¥ê´€", "ì„ ê±°", "ì •ë‹¹", "ì™¸êµ", "êµ­ë°©", "ì•ˆë³´"],
    # "ì‚¬íšŒ": ["ë…¸ë™", "ì¸ê¶Œ", "ë³µì§€", "ë²”ì£„", "ê²½ì°°", "ê²€ì°°", "ì¬íŒ", "ì‚¬ê±´ì‚¬ê³ ", "ì•ˆì „", "ì¬ë‚œ"],
    # "ê²½ì œ": ["ê²½ì œ", "ê¸ˆìœµ", "ì¦ê¶Œ", "íˆ¬ì", "ê¸°ì—…", "ì‚°ì—…", "ë¬´ì—­", "ë¶€ë™ì‚°", "ê±´ì„¤", "ë¬¼ê°€"],
    # "ê¸°ìˆ ": ["IT", "ì¸ê³µì§€ëŠ¥", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´", "ë°˜ë„ì²´", "ë°ì´í„°", "í†µì‹ ", "ë¡œë´‡", "ì‚¬ì´ë²„ë³´ì•ˆ", "ë¸”ë¡ì²´ì¸"],
    # "ê³¼í•™": ["ê³¼í•™ê¸°ìˆ ", "ë¬¼ë¦¬í•™", "í™”í•™", "ìƒëª…ê³¼í•™", "ì§€êµ¬ê³¼í•™", "ì²œë¬¸í•™", "ìš°ì£¼", "ì—°êµ¬ê°œë°œ", "í•™ìˆ ì§€", "ì‹¤í—˜"],
    # "ê±´ê°•": ["ê±´ê°•", "ì§ˆë³‘", "ì˜ë£Œ", "ë³‘ì›", "ì˜ì•½í’ˆ", "ë°±ì‹ ", "ì˜ì–‘", "ìš´ë™", "ì •ì‹ ê±´ê°•", "ê³µì¤‘ë³´ê±´"],
    # "êµìœ¡": ["êµìœ¡", "í•™êµ", "ëŒ€í•™", "ì…ì‹œ", "ìˆ˜ëŠ¥", "êµì‚¬", "í•™ìƒ", "í•™ì›", "í‰ìƒêµìœ¡", "ì˜¨ë¼ì¸êµìœ¡"],
    # "ë¬¸í™”": ["ë¬¸í™”", "ë¬¸í•™", "ì˜ˆìˆ ", "ê³µì—°", "ì „ì‹œ", "ì „í†µë¬¸í™”", "ë¯¸ìˆ ", "ì˜í™”ì œ", "ì–¸ì–´", "ì¶•ì œ"],
    # "ì—°ì˜ˆ": ["ì—°ì˜ˆ", "ì˜í™”", "ë“œë¼ë§ˆ", "ìŒì•…", "K-pop", "ì•„ì´ëŒ", "ë°©ì†¡", "ì˜ˆëŠ¥", "ê²Œì„", "ì›¹íˆ°"],
    # "ìŠ¤í¬ì¸ ": ["ìŠ¤í¬ì¸ ", "ì¶•êµ¬", "ì•¼êµ¬", "ë†êµ¬", "ë°°êµ¬", "ê³¨í”„", "ì˜¬ë¦¼í”½", "ì›”ë“œì»µ", "eìŠ¤í¬ì¸ ", "ì²´ìœ¡"],
    # "ì—­ì‚¬": ["ì—­ì‚¬", "í•œêµ­ì‚¬", "ì„¸ê³„ì‚¬", "ê³ ëŒ€ì‚¬", "ê·¼í˜„ëŒ€ì‚¬", "ê³ ê³ í•™", "ì—­ì‚¬ì¸ë¬¼", "ì „ìŸì‚¬", "ë¬¸í™”ì¬", "ì—­ì‚¬êµìœ¡"],
    # "í™˜ê²½": ["í™˜ê²½", "ê¸°í›„ë³€í™”", "íƒ„ì†Œì¤‘ë¦½", "ì¬í™œìš©", "ì—ë„ˆì§€", "ëŒ€ê¸°ì˜¤ì—¼", "ìˆ˜ì§ˆì˜¤ì—¼", "ìƒíƒœê³„", "ìì—°ì¬í•´", "í™˜ê²½ì •ì±…"],
    # "ì—¬í–‰": ["ì—¬í–‰", "ê´€ê´‘", "êµ­ë‚´ì—¬í–‰", "í•´ì™¸ì—¬í–‰", "í˜¸í…”", "í•­ê³µ", "êµí†µ", "ë§›ì§‘", "ì¶•ì œì—¬í–‰", "ì—¬í–‰í›„ê¸°"],
    # "ìƒí™œ": ["ìƒí™œ", "ìš”ë¦¬", "íŒ¨ì…˜", "ë·°í‹°", "ì¸í…Œë¦¬ì–´", "ë°˜ë ¤ë™ë¬¼", "ì·¨ë¯¸", "ìš´ë™", "ì›ì˜ˆ", "ë¼ì´í”„ìŠ¤íƒ€ì¼"],
    # "ê°€ì •": ["ê°€ì •", "ì—°ì• ", "ê²°í˜¼", "ì‹ í˜¼", "ìœ¡ì•„", "ìë…€êµìœ¡", "ê°€ì¡±ê´€ê³„", "ë¶€ë¶€", "ë¶€ëª¨", "ì²­ì†Œë…„"],
    # "ì¢…êµ": ["ì¢…êµ", "ê¸°ë…êµ", "ë¶ˆêµ", "ì²œì£¼êµ", "ì´ìŠ¬ëŒ", "ì¢…êµí–‰ì‚¬", "ì¢…êµê°ˆë“±", "ì‹ ì•™", "ëª…ìƒ", "ì˜ì„±"],
    # "ì² í•™": ["ì² í•™", "ìœ¤ë¦¬", "ì¸ë¬¸í•™", "ì •ì¹˜ì² í•™", "ì‚¬íšŒì² í•™", "ì„œì–‘ì² í•™", "ë™ì–‘ì² í•™", "í˜•ì´ìƒí•™", "ë…¼ë¦¬í•™", "ì¡´ì¬ë¡ "]
}

# ====== ë³¸ë¬¸ í¬ë¡¤ë§ í›„ ì „ì²˜ë¦¬ =====
def clean_title(raw_title):
    # HTML íƒœê·¸ ì œê±°
    text = BeautifulSoup(raw_title, "html.parser").get_text()
    return text

def clean_body_soup(soup):
    remove_targets = [
        # ì´ë¯¸ì§€/ì˜ìƒ/ê´‘ê³ /ëŒ“ê¸€ ë“± ë¶ˆí•„ìš” ìš”ì†Œ
        ("class", "end_photo_org"), ("class", "vod_player_wrap"),
        ("id", "video_area"), ("class", "vod_area"),
        ("class", "reporter_area"), ("class", "copyright"),
        ("class", "promotion"), ("class", "source"),
        ("class", "caption"), ("class", "byline"),
        ("class", "related_articles"), ("id", "scroll_sns"),
        ("class", "sns_share"), ("class", "reply"),
        ("id", "comment"), ("class", "ad"),
        ("class", "categorize"), ("class", "artical-btm")
    ]
    for attr, val in remove_targets:
        for tag in soup.find_all(attrs={attr: val}):
            tag.decompose()
    return soup

def clean_body(text):
    if not text:
        return ""
    # 1. ì´ë©”ì¼ ì œê±°
    text = re.sub(r'\S+@\S+', '', text)
    # 2. URL ì œê±° (http/https í¬í•¨)
    text = re.sub(r'http\S+|www\S+', '', text)
    # 3. ì œë³´/ì¶œì²˜/êµ¬ë… ì•ˆë‚´/ê´‘ê³ /ëŒ“ê¸€ ë¬¸êµ¬ ì œê±° (íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
    text = re.sub(r'^[â–¶â˜â– â–·â€»â—†â—‡â—â—‹â–²â–³â–¡â–£\-\+].*$', '', text, flags=re.MULTILINE)   # ê¸°í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
    text = re.sub(r'^\[ì‚¬ì§„ ì¶œì²˜.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^â–  ì œë³´í•˜ê¸°.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'ë„¤ì´ë²„.*êµ¬ë….*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(ê´€ë ¨ê¸°ì‚¬.*|ê´‘ê³ ì•ˆë‚´.*|ì œë³´.*)$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(SNS.*ë³´ë‚´ê¸°.*|ëŒ“ê¸€.*|ê¸°ìí˜ì´ì§€.*|ì €ì‘ê¶Œì.*|ë¬´ë‹¨ì „ì¬.*)$', '', text, flags=re.MULTILINE)
    # 4. ê¸°ìëª… / ìŠ¹ì¸Â·ìˆ˜ì •ì‹œê°„ ì œê±°
    text = re.sub(r'^\s*ê¸°ìëª….*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*[ê°€-í£]+\s?ê¸°ì.*$', '', text, flags=re.MULTILINE) 
    text = re.sub(r'^\s*ìŠ¹ì¸ì‹œê°„.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ìˆ˜ì •ì‹œê°„.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì…ë ¥.*$', '', text, flags=re.MULTILINE)
    # 5. ëŒ“ê¸€/ë¡œê·¸ì¸/ì‚­ì œ ì•ˆë‚´ ì œê±°
    text = re.sub(r'^\s*ëŒ“ê¸€.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì‚­ì œ.*$', '', text, flags=re.MULTILINE)        # "ì‚­ì œ"
    text = re.sub(r'^\s*ë‹«ê¸°.*$', '', text, flags=re.MULTILINE)        # "ë‹«ê¸°"
    text = re.sub(r'^\s*ë³µêµ¬.*ì—†ìŠµë‹ˆë‹¤.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ìˆ˜ì •.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë¹„ë°€ë²ˆí˜¸.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë‚´ ëŒ“ê¸€.*$', '', text, flags=re.MULTILINE)
    # 6. ê³µìœ /ìŠ¤í¬ë©/ë³´ë‚´ê¸° ì œê±°
    text = re.sub(r'^(.*ë³´ë‚´ê¸°.*)$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(.*ìŠ¤í¬ë©.*)$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(.*ê³µìœ .*)$', '', text, flags=re.MULTILINE)
    # 7. ì œë³´/ì—°ë½ì²˜ (ì´ë©”ì¼:, ì¹´ì¹´ì˜¤í†¡:, @ë…¸ì»·ë‰´ìŠ¤, ì‚¬ì´íŠ¸:)
    text = re.sub(r'^\s*ì´ë©”ì¼\s*:.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì¹´ì¹´ì˜¤í†¡\s*:.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*@ë…¸ì»·ë‰´ìŠ¤.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì‚¬ì´íŠ¸\s*:.*$', '', text, flags=re.MULTILINE)
    # 8. ì¶”ê°€: ì‚­ì œ ì•ˆë‚´/ì…ë ¥ì°½/í°íŠ¸ì¡°ì ˆ
    text = re.sub(r'^\s*ê·¸ë˜ë„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë³¸ë¬¸\s*/\s*400.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë°”ë¡œê°€ê¸°.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ê°€$', '', text, flags=re.MULTILINE)  # 'ê°€' ë‹¨ë… ë¼ì¸

    # 9. JSON íŒŒì‹± ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ëŠ” íŠ¹ìˆ˜ Unicode ë¬¸ì ì œê±°
    text = re.sub(r'[â—†â—‡â—â—‹â–²â–³â–¡â–£â™¦â™§â™ â™¥â—¯â—â—â—‘]', '', text)  # Unicode ë„í˜• ê¸°í˜¸ë“¤
    text = re.sub(r'[âš«âšªâ¬›â¬œ]', '', text)  # ê¸°íƒ€ ê¸°í•˜í•™ì  ë„í˜•
    text = re.sub(r'[âœ“âœ”âœ•âœ–âœ—]', '', text)  # ì²´í¬/X ë§ˆí¬ë“¤

    # 10. ê³µë°± ì •ë¦¬
    text = re.sub(r'\n+', '\n', text).strip()

    return text

# ====== ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ ======
def get_news(keyword, start=1, display=5):
    encText = urllib.parse.quote(str(keyword))
    url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&start={start}&display={display}&sort=sim"
    
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", client_id)
    request.add_header("X-Naver-Client-Secret", client_secret)
    
    response = urllib.request.urlopen(request)
    if response.getcode() == 200:
        response_body = response.read()
        return json.loads(response_body.decode('utf-8'))["items"]
    else:
        print("Error Code:" + str(response.getcode()))
        return []

# ====== ë³¸ë¬¸ í¬ë¡¤ë§ ======
def get_body(url):
    try:
        print(f"[{threading.current_thread().name}] ìš”ì²­ ì‹œì‘ â†’ {url}")
        res = requests.get(url, timeout=5, headers={"User-Agent":"Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")
        article = soup.find("article")
        if article:
            article = clean_body_soup(article)
            return clean_body(article.get_text(separator="\n").strip())
        else:
            return ""
    except Exception as e:
        print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨:", e)
        return ""

# ====== ìš”ì•½ ê¸°ëŠ¥ (KoBART ëª¨ë¸) ======
# ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ë° Lock
_summarization_service = None
_service_lock = threading.Lock()

def get_global_summarization_service():
    """Thread-safe ì‹±ê¸€í†¤ ìš”ì•½ ì„œë¹„ìŠ¤"""
    global _summarization_service
    if _summarization_service is None:
        with _service_lock:
            if _summarization_service is None:
                print("ğŸ”„ ìš”ì•½ ëª¨ë¸ ì „ì—­ ì´ˆê¸°í™” ì¤‘...")
                _summarization_service = get_summarization_service()
                if _summarization_service.pipe:
                    print("âœ… ìš”ì•½ ëª¨ë¸ ì „ì—­ ì´ˆê¸°í™” ì™„ë£Œ!")
                else:
                    print("âŒ ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨!")
    return _summarization_service

def summarize_text(text):
    """Thread-safe KoBART ëª¨ë¸ ìš”ì•½"""
    if not text or len(text.strip()) < 50:
        return "ìš”ì•½í•  ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."

    # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ë™ì  ê¸¸ì´ ì¡°ì •
    text_len = len(text.strip())
    if text_len < 200:
        max_length = min(text_len // 2, 80)
        min_length = min(max_length // 2, 30)
    else:
        max_length = min(text_len // 3, 150)
        min_length = min(max_length // 3, 50)

    try:
        # Thread-safe ëª¨ë¸ ì ‘ê·¼
        with _service_lock:
            summarization_service = get_global_summarization_service()
            if not summarization_service or not summarization_service.pipe:
                return "ìš”ì•½ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            summary = summarization_service.summarize_single(
                text,
                max_length=max_length,
                min_length=min_length
            )

        return summary if summary else "ìš”ì•½ ì‹¤íŒ¨"
    except Exception as e:
        print(f"ìš”ì•½ ì˜¤ë¥˜: {e}")
        return f"ìš”ì•½ ì˜¤ë¥˜: {str(e)[:30]}..."

# ====== ëŒ€ë¶„ë¥˜ì— ë”°ë¥¸ ì¤‘ë¶„ë¥˜ ë³‘ë ¬ ì²˜ë¦¬ =====
def process_keyword(category, keyword):
    """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½"""
    from core.mysql_db import SessionLocal
    session = SessionLocal()
    items = get_news(keyword, start=1, display=5)
    results = []

    for item in items:
        url = item["link"]
        
        if session.query(News).filter_by(url=url).first():
            print(f"â­ï¸ ìŠ¤í‚µ (ì´ë¯¸ ì¡´ì¬): {url}")
            continue
        
        print(f"[{category}-{keyword}] ì²˜ë¦¬ ì¤‘: {clean_title(item['title'])}")

        # ë³¸ë¬¸ ì¶”ì¶œ
        body = get_body(item["link"])

        # ìš”ì•½ ìƒì„±
        summary = ""
        if body:
            print(f"  â†’ ìš”ì•½ ìƒì„± ì¤‘... (ë³¸ë¬¸ ê¸¸ì´: {len(body)}ì)")
            summary = summarize_text(body)
            print(f"  â†’ ìš”ì•½ ì™„ë£Œ: {summary[:50]}...")
        else:
            print(f"  â†’ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")

        published_at = parse_pub_date(item.get("pubDate"))
        result = {
            "category": category,
            "keyword": keyword,
            "title": clean_title(item["title"]),
            "link": item["link"],
            "originallink": item.get("originallink"),
            "body": body,
            "summary": summary,
            "created_at": datetime.now().isoformat(),
            "published_at": published_at
        }

        save_to_db(result)

        results.append(result)

    return results


def parse_pub_date(raw_pubdate):
    """ë„¤ì´ë²„ API pubDate ë¬¸ìì—´ì„ ISO8601ë¡œ ë³€í™˜"""
    if not raw_pubdate:
        return None

    try:
        dt = parsedate_to_datetime(raw_pubdate)
        if dt.tzinfo:
            return dt.isoformat()
        return dt.replace(tzinfo=datetime.timezone.utc).isoformat()
    except Exception:
        return raw_pubdate

# ====== db ì €ì¥ =======
def save_to_db(item):
    from core.mysql_db import SessionLocal
    session = SessionLocal()
    try:
        category_id = CATEGORY_MAP.get(item["category"])
        if not category_id:
            print(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì‹¤íŒ¨: {item['category']}")
            return
        pub_dt = None
        if item.get("published_at"):
            try:
                pub_dt = datetime.fromisoformat(item["published_at"].replace("Z", "+00:00"))
            except Exception:
                pass
        news = News(
            category_id=category_id,
            title=item["title"],
            url=item["link"],
            summary=item["summary"][:1000],
            published_at=pub_dt
        )
        session.add(news)
        session.commit()
        print(f"âœ… ì €ì¥ ì„±ê³µ: {news.title[:30]}...")

    except IntegrityError:
        session.rollback()
        print(f"âš ï¸ ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µ: {item['link']}")
    finally:
        session.close()


# ====== JSON ì €ì¥ ======
def save_to_json(all_results, filename=None):
    """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"news_crawled_{timestamp}_2.json"

    json_data = {
        "created_at": datetime.now().isoformat(),
        "total_count": len(all_results),
        "categories": list(set(r['category'] for r in all_results)),
        "news": all_results
    }

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ë‰´ìŠ¤ ì €ì¥ë¨")
        return filename
    except Exception as e:
        print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

# ====== ë©”ì¸ ì‹¤í–‰ ======
def main():
    print("ğŸ¤– ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ìš”ì•½ ì‹œì‘!")
    print(f"ğŸ“‚ ì²˜ë¦¬ ì¹´í…Œê³ ë¦¬: {len(CATEGORIES)}ê°œ")
    print("=" * 60)

    all_results = []  # ì „ì²´ ê²°ê³¼ ì €ì¥ìš©

    # ëª¨ë¸ ì‚¬ì „ ë¡œë”© (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ)
    print("\nğŸ”„ ìš”ì•½ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘...")
    get_global_summarization_service()

    # ì „ì²´ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
    for category, keywords in CATEGORIES.items():
        print(f"\nğŸ“° [{category}] ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘...")
        category_results = []

        # EC2 t2.xlarge ë‹¤ì¤‘ ì„œë¹„ìŠ¤ í™˜ê²½ ìµœì í™” (Backend, DB ë“±ê³¼ ë¦¬ì†ŒìŠ¤ ê³µìœ )
        optimal_workers = min(2, len(keywords))  # ë³´ìˆ˜ì  ì„¤ì •
        with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
            print(f"  ğŸš€ {len(keywords)}ê°œ í‚¤ì›Œë“œë¥¼ {optimal_workers}ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì²˜ë¦¬")

            futures = [executor.submit(process_keyword, category, kw) for kw in keywords]

            for i, future in enumerate(as_completed(futures), 1):
                try:
                    results = future.result()
                    category_results.extend(results)
                    all_results.extend(results)
                    print(f"    âœ… [{i}/{len(keywords)}] ì™„ë£Œ: {len(results)}ê°œ ë‰´ìŠ¤")
                except Exception as e:
                    print(f"    âŒ [{i}/{len(keywords)}] ì—ëŸ¬: {e}")

        print(f"  ğŸ“Š [{category}] ì™„ë£Œ: {len(category_results)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")

    print(f"\nğŸ‰ ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ìš”ì•½ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(all_results)}ê°œ")

    # ìµœì¢… JSON ì €ì¥
    print(f"\n{'=' * 60}")
    print("ğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
    saved_file = save_to_json(all_results)

    if saved_file:
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“„ ì €ì¥ íŒŒì¼: {saved_file}")
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_results)}ê°œ ë‰´ìŠ¤")

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for result in all_results:
            cat = result['category']
            category_stats[cat] = category_stats.get(cat, 0) + 1

        print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
        for cat, count in category_stats.items():
            print(f"  - {cat}: {count}ê°œ")

        # Qdrant ì €ì¥ ë¡œì§ (í˜„ì¬ ë¹„í™œì„±í™”)
        # try:
        #     vectorizer = NewsVectorizer()
        #     stats = asyncio.run(vectorizer.vectorize_and_save_batch(all_results))
        #     print(
        #         f"âœ… Qdrant ì €ì¥ ì™„ë£Œ: ì´ {stats['embedded']}ê°œ ë²¡í„° ì €ì¥"
        #         f" (ìš”ì²­ {stats['processed']}ê°œ, ìŠ¤í‚µ {stats['skipped']}ê°œ)"
        #     )
        # except Exception as exc:
        #     print(f"âŒ ë‰´ìŠ¤ ë²¡í„°í™” ì‹¤íŒ¨: {exc}")

    else:
        print("âŒ ì €ì¥ ì‹¤íŒ¨!")

if __name__ == "__main__":
    main()

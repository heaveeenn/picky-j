import os
import sys
import urllib.request
import urllib.parse
import json
import requests
import asyncio
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import re
from email.utils import parsedate_to_datetime
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import sessionmaker
from datetime import datetime
from .models import News

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from .summarization import get_summarization_service
from .vectorizer import NewsVectorizer  # Qdrant ì €ì¥ í™œì„±í™”

# ====== í™˜ê²½ ì„¤ì • ======
client_id = os.environ.get('CLIENT_ID')
client_secret = os.environ.get('CLIENT_SECRET')

CATEGORY_MAP = {
    "ì •ì¹˜": 1,
    "ì‚¬íšŒ": 2,
    "ê²½ì œ": 3,
    "ê¸°ìˆ ": 4,
    "ê³¼í•™": 5,
    "ê±´ê°•": 6,
    "êµìœ¡": 7,
    "ë¬¸í™”": 8,
    "ì—”í„°í…Œì¸ë¨¼íŠ¸": 9, 
    "ìŠ¤í¬ì¸ ": 10,
    "ì—­ì‚¬": 11,
    "í™˜ê²½": 12,
    "ì—¬í–‰": 13,
    "ìƒí™œ": 14,
    "ê°€ì •": 15,
    "ì¢…êµ": 16,
    "ì² í•™": 17,
}

CATEGORIES = {
  "ì •ì¹˜": ["ì •ë¶€", "ëŒ€í†µë ¹", "êµ­íšŒ", "ì´ë¦¬", "ì¥ê´€", "ì„ ê±°", "ì •ë‹¹", "ì™¸êµ", "êµ­ë°©", "ì•ˆë³´", "ì •ì±…", "ê°œí—Œ", "ë¹„ë¦¬"],
  "ì‚¬íšŒ": ["ë…¸ë™", "ì¸ê¶Œ", "ë³µì§€", "ë²”ì£„", "ê²½ì°°", "ê²€ì°°", "ì¬íŒ", "ì‚¬ê±´ì‚¬ê³ ", "ì•ˆì „", "ì¬ë‚œ", "ì‹œìœ„", "ê°ˆë“±", "ì‹¤ì—…"],
  "ê²½ì œ": ["ê²½ì œ", "ê¸ˆìœµ", "ì¦ê¶Œ", "íˆ¬ì", "ê¸°ì—…", "ì‚°ì—…", "ë¬´ì—­", "ë¶€ë™ì‚°", "ê±´ì„¤", "ë¬¼ê°€", "í™˜ìœ¨", "ê³ ìš©", "ë¬´ì—­í˜‘ì •", "ìŠ¤íƒ€íŠ¸ì—…"],
  "ê¸°ìˆ ": ["IT", "ì¸ê³µì§€ëŠ¥", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´", "ë°˜ë„ì²´", "ë°ì´í„°", "í†µì‹ ", "ë¡œë´‡", "ì‚¬ì´ë²„ë³´ì•ˆ", "ë¸”ë¡ì²´ì¸", "í´ë¼ìš°ë“œ", "ìŠ¤íƒ€íŠ¸ì—…", "ë©”íƒ€ë²„ìŠ¤", "5G"],
  "ê³¼í•™": ["ê³¼í•™ê¸°ìˆ ", "ë¬¼ë¦¬í•™", "í™”í•™", "ìƒëª…ê³¼í•™", "ì§€êµ¬ê³¼í•™", "ì²œë¬¸í•™", "ìš°ì£¼", "ì—°êµ¬ê°œë°œ", "ì‹¤í—˜", "ìœ ì „ì", "ì˜í•™ì—°êµ¬", "ê¸°í›„ê³¼í•™", "ì‹ ì†Œì¬"],
  "ê±´ê°•": ["ê±´ê°•", "ì§ˆë³‘", "ì˜ë£Œ", "ë³‘ì›", "ì˜ì•½í’ˆ", "ë°±ì‹ ", "ì˜ì–‘", "ìš´ë™", "ì •ì‹ ê±´ê°•", "ê³µì¤‘ë³´ê±´", "ì˜ˆë°©", "ë‹¤ì´ì–´íŠ¸"],
  "êµìœ¡": ["êµìœ¡", "í•™êµ", "ëŒ€í•™", "ì…ì‹œ", "ìˆ˜ëŠ¥", "êµì‚¬", "í•™ìƒ", "í•™ì›", "í‰ìƒêµìœ¡", "ì˜¨ë¼ì¸êµìœ¡", "ì¥í•™ê¸ˆ", "êµê³¼ì„œ", "êµìœ¡ì •ì±…"],
  "ë¬¸í™”": ["ë¬¸í™”", "ë¬¸í•™", "ì˜ˆìˆ ", "ê³µì—°", "ì „ì‹œ", "ì „í†µë¬¸í™”", "ë¯¸ìˆ ", "ì˜í™”ì œ", "ì–¸ì–´", "ì¶•ì œ", "ì°½ì‘", "ì˜ˆìˆ ê°€"],
  "ì—”í„°í…Œì¸ë¨¼íŠ¸": ["ì—°ì˜ˆ", "ì˜í™”", "ë“œë¼ë§ˆ", "ìŒì•…", "K-pop", "ì•„ì´ëŒ", "ë°©ì†¡", "ì˜ˆëŠ¥", "ê²Œì„", "ì›¹íˆ°", "OTT", "íŒ¬ë¤", "ìŠ¤íƒ€"],
  "ìŠ¤í¬ì¸ ": ["ìŠ¤í¬ì¸ ", "ì¶•êµ¬", "ì•¼êµ¬", "ë†êµ¬", "ë°°êµ¬", "ê³¨í”„", "ì˜¬ë¦¼í”½", "ì›”ë“œì»µ", "eìŠ¤í¬ì¸ ", "ì²´ìœ¡", "í…Œë‹ˆìŠ¤", "ë§ˆë¼í†¤", "ì„ ìˆ˜ë‹¨"],
  "ì—­ì‚¬": ["ì—­ì‚¬", "í•œêµ­ì‚¬", "ì„¸ê³„ì‚¬", "ê³ ëŒ€ì‚¬", "ê·¼í˜„ëŒ€ì‚¬", "ê³ ê³ í•™", "ì—­ì‚¬ì¸ë¬¼", "ì „ìŸì‚¬", "ë¬¸í™”ì¬", "ì—­ì‚¬êµìœ¡", "ë…ë¦½ìš´ë™", "ìœ ì "],
  "í™˜ê²½": ["í™˜ê²½", "ê¸°í›„ë³€í™”", "íƒ„ì†Œì¤‘ë¦½", "ì¬í™œìš©", "ì—ë„ˆì§€", "ëŒ€ê¸°ì˜¤ì—¼", "ìˆ˜ì§ˆì˜¤ì—¼", "ìƒíƒœê³„", "ìì—°ì¬í•´", "í™˜ê²½ì •ì±…", "ë¯¸ì„¸ë¨¼ì§€", "ì¹œí™˜ê²½", "ì§€ì†ê°€ëŠ¥ì„±"],
  "ì—¬í–‰": ["ì—¬í–‰", "ê´€ê´‘", "êµ­ë‚´ì—¬í–‰", "í•´ì™¸ì—¬í–‰", "í˜¸í…”", "í•­ê³µ", "êµí†µ", "ë§›ì§‘", "ì—¬í–‰í›„ê¸°", "ì—¬í–‰ì •ë³´", "ë°°ë‚­ì—¬í–‰", "ê´€ê´‘ì§€"],
  "ìƒí™œ": ["ìƒí™œ", "ìš”ë¦¬", "íŒ¨ì…˜", "ë·°í‹°", "ì¸í…Œë¦¬ì–´", "ë°˜ë ¤ë™ë¬¼", "ì·¨ë¯¸", "ìš´ë™", "ì›ì˜ˆ", "ë¼ì´í”„ìŠ¤íƒ€ì¼", "ì†Œë¹„", "ì‡¼í•‘", "ì„œë¹„ìŠ¤"],
  "ê°€ì •": ["ê°€ì •", "ì—°ì• ", "ê²°í˜¼", "ì‹ í˜¼", "ìœ¡ì•„", "ìë…€êµìœ¡", "ê°€ì¡±ê´€ê³„", "ë¶€ë¶€", "ë¶€ëª¨", "ì²­ì†Œë…„", "ê°€ì‚¬", "ëŒë´„"],
  "ì¢…êµ": ["ì¢…êµ", "ê¸°ë…êµ", "ë¶ˆêµ", "ì²œì£¼êµ", "ì´ìŠ¬ëŒ", "ì¢…êµí–‰ì‚¬", "ì¢…êµê°ˆë“±", "ì‹ ì•™", "ëª…ìƒ", "ì˜ì„±", "ì‚¬ì°°", "êµíšŒ"],
  "ì² í•™": ["ì² í•™", "ìœ¤ë¦¬", "ì¸ë¬¸í•™", "ì •ì¹˜ì² í•™", "ì‚¬íšŒì² í•™", "ë™ì–‘ì² í•™", "ì„œì–‘ì² í•™", "ê°€ì¹˜ê´€", "ë„ë•", "ì‚¬ìƒ", "ì² í•™ì", "ì§„ë¦¬"]
}


# ====== ë³¸ë¬¸ í¬ë¡¤ë§ í›„ ì „ì²˜ë¦¬ =====
def clean_title(raw_title):
    # HTML íƒœê·¸ ì œê±°
    text = BeautifulSoup(raw_title, "html.parser").get_text()
    return text

def clean_body_soup(soup):
    remove_targets = [
        # ì´ë¯¸ì§€/ì˜ìƒ/ê´‘ê³ /ëŒ“ê¸€ ë“± ë¶ˆí•„ìš” ìš”ì†Œ
        ("class", "end_photo_org"), ("class", "vod_player_wrap"),
        ("id", "video_area"), ("class", "vod_area"),
        ("class", "reporter_area"), ("class", "copyright"),
        ("class", "promotion"), ("class", "source"),
        ("class", "caption"), ("class", "byline"),
        ("class", "related_articles"), ("id", "scroll_sns"),
        ("class", "sns_share"), ("class", "reply"),
        ("id", "comment"), ("class", "ad"),
        ("class", "categorize"), ("class", "artical-btm")
    ]
    for attr, val in remove_targets:
        for tag in soup.find_all(attrs={attr: val}):
            tag.decompose()
    return soup

def clean_body(text):
    if not text:
        return ""
    # 1. ì´ë©”ì¼ ì œê±°
    text = re.sub(r'\S+@\S+', '', text)
    # 2. URL ì œê±° (http/https í¬í•¨)
    text = re.sub(r'http\S+|www\S+', '', text)
    # 3. ì œë³´/ì¶œì²˜/êµ¬ë… ì•ˆë‚´/ê´‘ê³ /ëŒ“ê¸€ ë¬¸êµ¬ ì œê±° (íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
    text = re.sub(r'^[â–¶â˜â– â–·â€»â—†â—‡â—â—‹â–²â–³â–¡â–£\-\+].*$', '', text, flags=re.MULTILINE)   # ê¸°í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ì¤„
    text = re.sub(r'^\[ì‚¬ì§„ ì¶œì²˜.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^â–  ì œë³´í•˜ê¸°.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'ë„¤ì´ë²„.*êµ¬ë….*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(ê´€ë ¨ê¸°ì‚¬.*|ê´‘ê³ ì•ˆë‚´.*|ì œë³´.*)$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(SNS.*ë³´ë‚´ê¸°.*|ëŒ“ê¸€.*|ê¸°ìí˜ì´ì§€.*|ì €ì‘ê¶Œì.*|ë¬´ë‹¨ì „ì¬.*)$', '', text, flags=re.MULTILINE)
    # 4. ê¸°ìëª… / ìŠ¹ì¸Â·ìˆ˜ì •ì‹œê°„ ì œê±°
    text = re.sub(r'^\s*ê¸°ìëª….*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*[ê°€-í£]+\s?ê¸°ì.*$', '', text, flags=re.MULTILINE) 
    text = re.sub(r'^\s*ìŠ¹ì¸ì‹œê°„.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ìˆ˜ì •ì‹œê°„.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì…ë ¥.*$', '', text, flags=re.MULTILINE)
    # 5. ëŒ“ê¸€/ë¡œê·¸ì¸/ì‚­ì œ ì•ˆë‚´ ì œê±°
    text = re.sub(r'^\s*ëŒ“ê¸€.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì‚­ì œ.*$', '', text, flags=re.MULTILINE)        # "ì‚­ì œ"
    text = re.sub(r'^\s*ë‹«ê¸°.*$', '', text, flags=re.MULTILINE)        # "ë‹«ê¸°"
    text = re.sub(r'^\s*ë³µêµ¬.*ì—†ìŠµë‹ˆë‹¤.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ìˆ˜ì •.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë¹„ë°€ë²ˆí˜¸.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë‚´ ëŒ“ê¸€.*$', '', text, flags=re.MULTILINE)
    # 6. ê³µìœ /ìŠ¤í¬ë©/ë³´ë‚´ê¸° ì œê±°
    text = re.sub(r'^(.*ë³´ë‚´ê¸°.*)$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(.*ìŠ¤í¬ë©.*)$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^(.*ê³µìœ .*)$', '', text, flags=re.MULTILINE)
    # 7. ì œë³´/ì—°ë½ì²˜ (ì´ë©”ì¼:, ì¹´ì¹´ì˜¤í†¡:, @ë…¸ì»·ë‰´ìŠ¤, ì‚¬ì´íŠ¸:)
    text = re.sub(r'^\s*ì´ë©”ì¼\s*:.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì¹´ì¹´ì˜¤í†¡\s*:.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*@ë…¸ì»·ë‰´ìŠ¤.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ì‚¬ì´íŠ¸\s*:.*$', '', text, flags=re.MULTILINE)
    # 8. ì¶”ê°€: ì‚­ì œ ì•ˆë‚´/ì…ë ¥ì°½/í°íŠ¸ì¡°ì ˆ
    text = re.sub(r'^\s*ê·¸ë˜ë„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë³¸ë¬¸\s*/\s*400.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ë°”ë¡œê°€ê¸°.*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*ê°€$', '', text, flags=re.MULTILINE)  # 'ê°€' ë‹¨ë… ë¼ì¸

    # 9. JSON íŒŒì‹± ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ëŠ” íŠ¹ìˆ˜ Unicode ë¬¸ì ì œê±°
    text = re.sub(r'[â—†â—‡â—â—‹â–²â–³â–¡â–£â™¦â™§â™ â™¥â—¯â—â—â—‘]', '', text)  # Unicode ë„í˜• ê¸°í˜¸ë“¤
    text = re.sub(r'[âš«âšªâ¬›â¬œ]', '', text)  # ê¸°íƒ€ ê¸°í•˜í•™ì  ë„í˜•
    text = re.sub(r'[âœ“âœ”âœ•âœ–âœ—]', '', text)  # ì²´í¬/X ë§ˆí¬ë“¤

    # 10. ê³µë°± ì •ë¦¬
    text = re.sub(r'\n+', '\n', text).strip()

    return text

# ====== ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ ======
def get_news(keyword, start=1, display=5):
    encText = urllib.parse.quote(str(keyword))
    url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&start={start}&display={display}&sort=sim"
    
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", client_id)
    request.add_header("X-Naver-Client-Secret", client_secret)
    
    response = urllib.request.urlopen(request)
    if response.getcode() == 200:
        response_body = response.read()
        return json.loads(response_body.decode('utf-8'))["items"]
    else:
        print("Error Code:" + str(response.getcode()))
        return []

# ====== ë³¸ë¬¸ í¬ë¡¤ë§ ======
def is_good_content(text):
    """í…ìŠ¤íŠ¸ê°€ ì‹¤ì œ ë‰´ìŠ¤ ë³¸ë¬¸ì¸ì§€ ê°„ë‹¨ ê²€ì¦"""
    if len(text) < 100:
        return False

    # ë©”ë‰´ í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œì™¸
    menu_keywords = ["ì „ì²´ë©”ë‰´", "ê¸°ì‚¬ê²€ìƒ‰", "ë¡œê·¸ì¸", "facebook", "ê²€ìƒ‰", "ë‹«ê¸°"]
    menu_count = sum(1 for kw in menu_keywords if kw in text)
    if menu_count > 3:
        return False

    # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œì™¸ (ì‚¬ì´íŠ¸ ë„¤ë¹„ê²Œì´ì…˜)
    category_keywords = ["ëª¨ë°”ì¼Â·ê°€ì „", "ë°©ì†¡Â·í†µì‹ ", "ë°˜ë„ì²´Â·ë””ìŠ¤í”Œë ˆì´", "SWÂ·ë³´ì•ˆ", "ê¸ˆìœµ", "ì¦ê¶Œ"]
    category_count = sum(1 for kw in category_keywords if kw in text)
    if category_count > 2:
        return False

    return True

def try_alternative_selectors(soup):
    """ëŒ€ì•ˆ ì„ íƒìë“¤ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„"""
    selectors = [
        ".article-body",
        ".content",
        "#content",
        ".news-content",
        ".article-content"
    ]

    candidates = []

    for selector in selectors:
        element = soup.select_one(selector)
        if element:
            element_text = clean_body(clean_body_soup(element).get_text(separator="\n").strip())
            if is_good_content(element_text):
                candidates.append((element_text, len(element_text)))

    # ì í•©í•œ í›„ë³´ ì¤‘ ê°€ì¥ ê¸´ ê²ƒ ë°˜í™˜
    if candidates:
        best_text, _ = max(candidates, key=lambda x: x[1])
        return best_text

    return ""

def get_body(url):
    try:
        print(f"[{threading.current_thread().name}] ìš”ì²­ ì‹œì‘ â†’ {url}")
        res = requests.get(url, timeout=5, headers={"User-Agent":"Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")

        # 1. ê¸°ì¡´ ë°©ì‹ ë¨¼ì € ì‹œë„ (ë¹ ë¦„)
        article = soup.find("article")
        if article:
            cleaned_article = clean_body_soup(article)
            text = clean_body(cleaned_article.get_text(separator="\n").strip())
            if is_good_content(text):
                return text

        # 2. ê¸°ì¡´ ë°©ì‹ ì‹¤íŒ¨ì‹œ ëŒ€ì•ˆ ì„ íƒì ì‹œë„
        alternative_text = try_alternative_selectors(soup)
        if alternative_text:
            return alternative_text

        # 3. ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
        return ""

    except Exception as e:
        print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨:", e)
        return ""

# ====== ìš”ì•½ ê¸°ëŠ¥ (KoBART ëª¨ë¸) ======
# ì „ì—­ ëª¨ë¸ ì„œë¹„ìŠ¤ ë° Lock
_summarization_service = None
_service_lock = threading.Lock()

# ====== ì‹¤ì‹œê°„ ë²¡í„°í™” í ======
_vectorization_queue = []
_vectorization_lock = threading.Lock()
_vectorizer = None

def get_global_vectorizer():
    """Thread-safe ì‹±ê¸€í†¤ ë²¡í„°í™” ì„œë¹„ìŠ¤"""
    global _vectorizer
    if _vectorizer is None:
        with _vectorization_lock:
            if _vectorizer is None:
                _vectorizer = NewsVectorizer()
    return _vectorizer

def process_vectorization_batch_async(batch_to_process, batch_id):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„°í™” ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        print(f"ğŸ”„ ë°°ì¹˜ ë²¡í„°í™” ì‹œì‘ (ë°°ì¹˜ #{batch_id}, {len(batch_to_process)}ê°œ)")
        vectorizer = get_global_vectorizer()
        stats = asyncio.run(vectorizer.vectorize_and_save_batch(batch_to_process))
        print(f"âœ… ë°°ì¹˜ #{batch_id} ë²¡í„°í™” ì™„ë£Œ: {stats['embedded']}ê°œ ì €ì¥")
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ #{batch_id} ë²¡í„°í™” ì‹¤íŒ¨: {e}")

# ë°°ì¹˜ ì¹´ìš´í„° (ë°°ì¹˜ ID ìƒì„±ìš©)
_batch_counter = 0
_batch_counter_lock = threading.Lock()

def add_to_vectorization_queue(news_item):
    """ë‰´ìŠ¤ ì•„ì´í…œì„ ë²¡í„°í™” íì— ì¶”ê°€í•˜ê³ , 16ê°œê°€ ë˜ë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„°í™” ì‹¤í–‰"""
    global _vectorization_queue, _batch_counter

    with _vectorization_lock:
        _vectorization_queue.append(news_item)
        current_count = len(_vectorization_queue)

        # 16ê°œê°€ ëª¨ì´ë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„°í™” ì‹¤í–‰
        if current_count >= 16:
            batch_to_process = _vectorization_queue[:16]
            _vectorization_queue = _vectorization_queue[16:]  # íì—ì„œ ì œê±°

            # ë°°ì¹˜ ID ìƒì„±
            with _batch_counter_lock:
                _batch_counter += 1
                batch_id = _batch_counter

            # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë²¡í„°í™” ì‹¤í–‰ (ë…¼ë¸”ë¡œí‚¹)
            vectorization_thread = threading.Thread(
                target=process_vectorization_batch_async,
                args=(batch_to_process, batch_id),
                daemon=True  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ í•¨ê»˜ ì¢…ë£Œ
            )
            vectorization_thread.start()
            print(f"ğŸš€ ë°°ì¹˜ #{batch_id} ë²¡í„°í™” ìŠ¤ë ˆë“œ ì‹œì‘ (16ê°œ) - í¬ë¡¤ë§ ê³„ì†...")

def flush_remaining_vectorization_queue():
    """ë‚¨ì€ íì˜ ëª¨ë“  ì•„ì´í…œì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë²¡í„°í™”í•˜ê³  ì™„ë£Œ ëŒ€ê¸°"""
    global _vectorization_queue, _batch_counter

    with _vectorization_lock:
        if _vectorization_queue:
            remaining_count = len(_vectorization_queue)
            batch_to_process = _vectorization_queue.copy()
            _vectorization_queue.clear()

            # ë°°ì¹˜ ID ìƒì„±
            with _batch_counter_lock:
                _batch_counter += 1
                batch_id = _batch_counter

            print(f"ğŸ”„ ìµœì¢… ë°°ì¹˜ ë²¡í„°í™” ì‹œì‘ (ë°°ì¹˜ #{batch_id}, {remaining_count}ê°œ)")

            # ìµœì¢… ë°°ì¹˜ëŠ” ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (ì™„ë£Œ ëŒ€ê¸° í•„ìš”)
            try:
                vectorizer = get_global_vectorizer()
                stats = asyncio.run(vectorizer.vectorize_and_save_batch(batch_to_process))
                print(f"âœ… ìµœì¢… ë°°ì¹˜ #{batch_id} ë²¡í„°í™” ì™„ë£Œ: {stats['embedded']}ê°œ ì €ì¥")
                return stats
            except Exception as e:
                print(f"âŒ ìµœì¢… ë°°ì¹˜ #{batch_id} ë²¡í„°í™” ì‹¤íŒ¨: {e}")
                return {"processed": 0, "embedded": 0, "skipped": 0}

    return {"processed": 0, "embedded": 0, "skipped": 0}

def get_global_summarization_service():
    """Thread-safe ì‹±ê¸€í†¤ ìš”ì•½ ì„œë¹„ìŠ¤"""
    global _summarization_service
    if _summarization_service is None:
        with _service_lock:
            if _summarization_service is None:
                print("ğŸ”„ ìš”ì•½ ëª¨ë¸ ì „ì—­ ì´ˆê¸°í™” ì¤‘...")
                _summarization_service = get_summarization_service()
                if _summarization_service.pipe:
                    print("âœ… ìš”ì•½ ëª¨ë¸ ì „ì—­ ì´ˆê¸°í™” ì™„ë£Œ!")
                else:
                    print("âŒ ìš”ì•½ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨!")
    return _summarization_service

def summarize_text(text):
    """Thread-safe KoBART ëª¨ë¸ ìš”ì•½"""
    if not text or len(text.strip()) < 50:
        return "ìš”ì•½í•  ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."

    # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ë™ì  ê¸¸ì´ ì¡°ì •
    text_len = len(text.strip())
    if text_len < 200:
        max_length = min(text_len // 2, 80)
        min_length = min(max_length // 2, 30)
    else:
        max_length = min(text_len // 3, 150)
        min_length = min(max_length // 3, 50)

    try:
        # Thread-safe ëª¨ë¸ ì ‘ê·¼
        with _service_lock:
            summarization_service = get_global_summarization_service()
            if not summarization_service or not summarization_service.pipe:
                return "ìš”ì•½ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            summary = summarization_service.summarize_single(
                text,
                max_length=max_length,
                min_length=min_length
            )

        return summary if summary else "ìš”ì•½ ì‹¤íŒ¨"
    except Exception as e:
        print(f"ìš”ì•½ ì˜¤ë¥˜: {e}")
        return f"ìš”ì•½ ì˜¤ë¥˜: {str(e)[:30]}..."

# ====== ëŒ€ë¶„ë¥˜ì— ë”°ë¥¸ ì¤‘ë¶„ë¥˜ ë³‘ë ¬ ì²˜ë¦¬ =====
def process_keyword(category, keyword):
    """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½"""
    from core.mysql_db import SessionLocal
    session = SessionLocal()

    try:
        items = get_news(keyword, start=1, display=5)
        results = []

        for item in items:
            url = item["link"]

            if session.query(News).filter_by(url=url).first():
                print(f"â­ï¸ ìŠ¤í‚µ (ì´ë¯¸ ì¡´ì¬): {url}")
                continue

            print(f"[{category}-{keyword}] ì²˜ë¦¬ ì¤‘: {clean_title(item['title'])}")

            # ë³¸ë¬¸ ì¶”ì¶œ
            body = get_body(item["link"])

            # ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
            if not body:
                print(f"  â†’ ìŠ¤í‚µ: ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨")
                continue

            # ìš”ì•½ ìƒì„±
            print(f"  â†’ ìš”ì•½ ìƒì„± ì¤‘... (ë³¸ë¬¸ ê¸¸ì´: {len(body)}ì)")
            summary = summarize_text(body)

            # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
            if not summary or "ìš”ì•½ ì‹¤íŒ¨" in summary or "ìš”ì•½ ì˜¤ë¥˜" in summary:
                print(f"  â†’ ìŠ¤í‚µ: ìš”ì•½ ìƒì„± ì‹¤íŒ¨ ({summary[:30]}...)")
                continue

            print(f"  â†’ ìš”ì•½ ì™„ë£Œ: {summary[:50]}...")

            published_at = parse_pub_date(item.get("pubDate"))
            result = {
                "category": category,
                "keyword": keyword,
                "title": clean_title(item["title"]),
                "link": item["link"],
                "originallink": item.get("originallink"),
                "body": body,
                "summary": summary,
                "created_at": datetime.now().isoformat(),
                "published_at": published_at
            }

            news_id = save_to_db(result)

            # ì‹¤ì‹œê°„ ë²¡í„°í™” íì— ì¶”ê°€ (ì •ìƒ ë‰´ìŠ¤ë§Œ)
            if news_id:
                result["id"] = news_id  # DBì—ì„œ ìƒì„±ëœ ID ì¶”ê°€
                add_to_vectorization_queue(result)

            results.append(result)

        return results

    finally:
        session.close()


def parse_pub_date(raw_pubdate):
    """ë„¤ì´ë²„ API pubDate ë¬¸ìì—´ì„ ISO8601ë¡œ ë³€í™˜"""
    if not raw_pubdate:
        return None

    try:
        dt = parsedate_to_datetime(raw_pubdate)
        if dt.tzinfo:
            return dt.isoformat()
        return dt.replace(tzinfo=datetime.timezone.utc).isoformat()
    except Exception:
        return raw_pubdate

# ====== db ì €ì¥ =======
def save_to_db(item):
    from core.mysql_db import SessionLocal
    session = SessionLocal()
    try:
        category_id = CATEGORY_MAP.get(item["category"])
        if not category_id:
            print(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì‹¤íŒ¨: {item['category']}")
            return None
        pub_dt = None
        if item.get("published_at"):
            try:
                pub_dt = datetime.fromisoformat(item["published_at"].replace("Z", "+00:00"))
            except Exception:
                pass
        news = News(
            category_id=category_id,
            title=item["title"],
            url=item["link"],
            summary=item["summary"][:1000],
            published_at=pub_dt
        )
        session.add(news)
        session.commit()
        print(f"âœ… ì €ì¥ ì„±ê³µ: {news.title[:30]}...")
        return news.id  # ìƒì„±ëœ ID ë°˜í™˜

    except IntegrityError:
        session.rollback()
        print(f"âš ï¸ ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µ: {item['link']}")
        return None
    finally:
        session.close()



# ====== ë©”ì¸ ì‹¤í–‰ ======
def main():
    print("ğŸ¤– ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ìš”ì•½ ì‹œì‘!")
    print(f"ğŸ“‚ ì²˜ë¦¬ ì¹´í…Œê³ ë¦¬: {len(CATEGORIES)}ê°œ")
    print("=" * 60)


    # ëª¨ë¸ ì‚¬ì „ ë¡œë”© (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ)
    print("\nğŸ”„ ìš”ì•½ ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘...")
    get_global_summarization_service()

    # ì „ì²´ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
    for category, keywords in CATEGORIES.items():
        print(f"\nğŸ“° [{category}] ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘...")
        category_results = []

        # EC2 t2.xlarge ë‹¤ì¤‘ ì„œë¹„ìŠ¤ í™˜ê²½ ìµœì í™” (Backend, DB ë“±ê³¼ ë¦¬ì†ŒìŠ¤ ê³µìœ )
        optimal_workers = min(2, len(keywords))  # ë³´ìˆ˜ì  ì„¤ì •
        with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
            print(f"  ğŸš€ {len(keywords)}ê°œ í‚¤ì›Œë“œë¥¼ {optimal_workers}ê°œ ìŠ¤ë ˆë“œë¡œ ë³‘ë ¬ ì²˜ë¦¬")

            futures = [executor.submit(process_keyword, category, kw) for kw in keywords]

            for i, future in enumerate(as_completed(futures), 1):
                try:
                    results = future.result()
                    category_results.extend(results)
                    print(f"    âœ… [{i}/{len(keywords)}] ì™„ë£Œ: {len(results)}ê°œ ë‰´ìŠ¤")
                except Exception as e:
                    print(f"    âŒ [{i}/{len(keywords)}] ì—ëŸ¬: {e}")

        print(f"  ğŸ“Š [{category}] ì™„ë£Œ: {len(category_results)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")

    print(f"\nğŸ‰ ì „ì²´ ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ìš”ì•½ ì™„ë£Œ!")

    # ë‚¨ì€ ë²¡í„°í™” í ì²˜ë¦¬
    print(f"\nğŸ”— ë‚¨ì€ ë²¡í„°í™” í ì²˜ë¦¬ ì¤‘...")
    final_stats = flush_remaining_vectorization_queue()
    if final_stats["processed"] > 0:
        print(f"âœ… ìµœì¢… ë²¡í„°í™” ì™„ë£Œ: ì´ {final_stats['embedded']}ê°œ ë²¡í„° ì €ì¥")

    print(f"\nğŸ‰ í¬ë¡¤ë§ ë° ë²¡í„°í™” ì™„ë£Œ!")

if __name__ == "__main__":
    main()
